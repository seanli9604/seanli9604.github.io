---
title: 'The (Stochastic) Chinese Room'
date: 2025-07-08
permalink: /posts/2025/07/blog-post-1/
tags:
  - Generative AI
  - Philosophy
  - Consciousness
---

Recently, I went down a rabbit hole of reading vaguely "pro-AI" subreddits, where people kept on claiming that large language models as they are today are somehow "conscious", or can be made to be conscious. Either this, or they claim that it is inevitable that by scaling up existing AI models, that they will *become* conscious (leading to a sci-fi AI takeover style scenario). I could just dismiss or ignore these people, but I realised something interesting; large language models can be thought of as a practical realisation of the *Chinese Room* philosophical thought experiment. So, I decided to write something around this topic, exploring exactly why the models we have today can't be thought of as possessing "human understanding" in any meaningful way. 

The Chinese Room
======

Suppose you are alone, in a sealed room. All around you is a massive collection of books, describing a detailed set of rules for providing an answer to any question written in Chinese, some blank pieces of paper, and a bunch of writing implements. Assume in this scenario that you don’t understand any Chinese, not even a single Chinese character. One day, through a strategically placed slot, you receive a question, written in Chinese. Now suppose that, perhaps because you have nothing better to do, you followed the rules described in these books step-by-step, until you managed to write a complete response to the question written in Chinese. Then, you insert the response through the same slot, and get another question, which you respond to in the same way. The process repeats itself an arbitrary number of times, with you receiving a question, transcribing a response using the rules, then inserting the response into the slot. 

Unbeknownst to you, the person who has been sending you those questions is fluent in Chinese. Given that you have repeatedly answered their questions correctly (or at least cogently), the person might conclude that you, the person in the sealed room, knows Chinese. But of course you do not, even if you had memorised the entire rule set described in all those books. To demonstrate this, suppose this person breaks you out of your confinement, and takes you into another room. In this room are a number of different objects, arranged on a table. The person presents you with a single Chinese character, “水” (meaning water) and tells you (in English) to point to the right object, a glass of water. The probability of you getting this right would be no greater than chance, because nowhere in the ruleset, which just concerns rules for reading and writing down characters, is there the association between “水” and a literal glass of water, since the latter is an object and not a Chinese character. 

This is a rephrasing of John Searle’s “[Chinese Room Argument](https://en.wikipedia.org/wiki/Chinese_room)”, where he argues that a computer, programmed simply to manipulate input symbols to give an output response, cannot be “conscious”, in the sense of being able to truly understand the inputs given to it. Just like how you, the person in the Chinese room, do not understand that “水” means water, even if you understood exactly how to respond to any question with that character in it. 

This argument was initially met with much criticism. In fact, when I first heard of it, I didn’t feel like it was very convincing either. Couldn’t you argue that, even if neither you nor the room understands Chinese, the combined system of “you-and-the-room” understands Chinese as a whole? This counterargument makes a lot of intuitive sense. After all, it’s hard to imagine that such a ruleset can even exist, without it implicitly lending you (the person applying the rules) with some understanding of Chinese. But, for reasons which will quickly become apparent, this simply reflects a lack of imagination, instead of any issues with the core point the argument is making. The main issue here is the idea of applying deterministic “rules”. The Chinese Room Argument was presented in 1980, at a time where the idea of “artificial intelligence” still revolved around rule-based symbol manipulation. When we dispense with the idea that any deterministic rule has to be followed, the argument is much clearer. 

Let’s suppose you are back in the sealed room, minus all the rule books. The only thing in front of you is a number of blank pages, some writing implements, and all 26 letters of the English alphabet. In this scenario, *you* are instead the Chinese speaker, and do not understand any English, not even a single word. Now, you receive a message through the slot. It is also a question, but this time, it is written in English. Being bored out of your mind and noting the similarities between the letters on the page and the 26 letters of the alphabet, you start scribbling down completely random letters as a response. Once you are satisfied, you place the response through the slot, and receive another message, with the process repeating as before.

Unbeknownst to you, you are the *luckiest person imaginable*. The completely random string of characters that you scribbled down somehow correspond to cogent answers for every single question given to you. Although the probability of this occurring is mind-bogglingly low, it is not zero. And in this scenario, you clearly still do not understand a single written word, including the questions that you responded to with random letters. In fact, you don’t even have to be in the room. Instead, a (very lucky) computer could have randomly generated the letters, and the outcome would be the same. 

The above seems like an extremely contrived and abstract scenario, a re-imagining of the infinite monkey theorem. One can easily make the claim that, if the person had just kept on sending through messages, then at any point they could receive complete gibberish, and the cover would be blown. However, the above argument could only be made if you assume that the probability of penning down any letter is uniformly distributed, or more generally speaking, *independent* of what letters you have written down previously. 

Ok, let’s present a third scenario. This time, you are again a Chinese speaker that do not understand English, and in a sealed room. In the room is a bunch of dice and the **objectively correct** [joint probability distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution) of all combinations of letters in the English language, up to say 1 million. You can imagine this as a comically large stack of paper, with a total of $$> 26^{1,000,000}$$ numbers written on it. When you receive a question, you compute from this the probability of the next letter, conditional on all letters in the question, and write down one letter with probability described by that conditional probability distribution, which you simulate by rolling a set number of dice. Keep in mind that you can do this using just simple arithmetic, albeit an inordinate amount of it. After writing down that first letter, you re-compute the probability of the next letter, this time not just conditional on the letters in the question, but also on the first letter that you’ve already written down. You repeat the process of computing conditional probabilities and writing down letters based on these probabilities, until you have written a complete response. Is the response still gibberish? Does the ability to compute probability distributions over letters give you any understanding of the question given to you?


LLMs As The Chinese Room
======

It should be clear now that this third scenario, unlike the previous two, is describing an idealised version of something that actually exists; large language models. Of course, there are some minor differences. For instance, LLMs are token-based and not character-based, and do not have access to this objectively correct joint probability distribution (it instead being learnt from data). It is also obviously not computationally tractable to learn from data a full joint distribution of all sequences of tokens, so various tricks are employed. But fundamentally, that’s what LLMs do. Their complexity serves to make this process physically possible with modern computational infrastructure, as well as more compatible with user demands, but does not make them categorically different from the person in this scenario. Except, there isn’t even a person there. It’s just a computer doing these calculations. So, it should be obvious that they are not conscious in any meaningful sense of the word “conscious”, and they have no understanding of the world in any meaningful sense of the word “understanding”. 

“But don’t humans learn statistical patterns from data in the same way?” Perhaps in the most shallow sense, but that analogy very quickly breaks down. Humans experience multiple sensory inputs; not just text, but also images, sounds, smells, taste and touch, in a time-dependent, embodied way. Unlike AI models, which are trained to minimise some loss function or maximise some sort of return, humans have a complex set of emotions, pushing and prodding our actions in all sorts of ways and making our subjective experience even more layered and multifaceted than what the senses themselves would suggest. If the brain indeed works analogously to a computer, then it must integrate all of this complexity together, using a statistical model “trained” by not just the cumulative sum of all these experiences, but billions of years of evolutionary priming, ingraining into us the basis for an instinctive understanding of how to manipulate our bodies through 3D space, how to condition our actions to stimuli, an instinctive distinction between the self and the external world, and so on. It is only from this massive foundation that human understanding is constructed; the tip of an immeasurably large iceberg.  

And what of our thoughts? Although people certainly *can* think in words, we are by no means restricted only to this. Personally, I’ve had language-based thoughts, usually a combination of an auditory “inner voice” with a “subtitle” of written text over it. But aside from those, I’ve also noted purely visual thoughts, either depicting a literal object or image, or some sort of abstract structure (think of diagrams, or maths). I also have “emotion”-based thoughts, which are associated with a particular sensation in the body, or thoughts that consist of music, which are associated with more complex emotional experiences. Keep in mind that this is just a description, recoverable from self-reflection, of atomised “thought-components” that I have. Actual thoughts that I have would involve all of the above (to some extent) happening at once, in a tightly integrated manner. When it comes to understanding concepts or solving problems, I fluidly and automatically shift between representations, based on how easy it is to express that problem in the representation. 

Obviously, I do not have access to other people’s thoughts, so I can only speak on my own behalf. But given that I can consciously distinguish between so many different modalities of thought (not even counting the subconscious processes), it would be reasonable to conclude that human thought and conceptual understanding is inherently multimodal. In fact, another piece of evidence suggesting this is the multimodal nature of how people appreciate artwork. For example, people usually do not only talk about novels strictly in terms of other text, but of the “imagery” evoked, the feelings that arise when as you read the story, and so on. Another feature of human thought is that they are *spontaneous and non-linear*. Thoughts do not occur as a singular output to sensory input, and do not require any objective or terminating point. The concept of “daydreaming” proves my point quite well here. The non-linearity of thought is also apparent, from the fact that people can jump associatively between many different topics, without needing to think in a “step-by-step” way. 

“That’s cool and all, but AI is very complex! The word “emergence” means that what happens in AI could be even more involved than my description of human thought, and make it the same or possibly better at understanding than humans!” I hear the AGI-believer say. Let’s suppose that I am actually convinced by this argument, and have started entertaining the possibility of LLMs being conscious, of actually “understanding” the world in a comparable way to humans. If LLMs are actually conscious, what would their “thoughts” look like? To have some idea of how to answer this question, let’s first eliminate all aspects of human experience that LLMs, in their current state, cannot possibly have. 

Firstly, they cannot experience time, being “aware” of only a continuity in data passed through it. It doesn’t matter if I send an LLM a message a second after it responds to my previous message, or a million years. To it, an identical amount of time has passed, because no actual computation (or any process analogous to thought) occurs, unless it is explicitly generating something. Second, they can only associate words (or tokens) with a limited subset of modalities. In the case of a “pure” LLM, they can only associate words with other words; not physical objects, not the feelings that the words might evoke, nothing other than the words themselves. Even “multimodal” models are multimodal only in the shallowest sense; able to associate text with 2D images with spoken language. A consequence of this is that an LLM also cannot innately tell if one user or multiple users are interacting with it, since it has no conception of a “user” (or of any object or agent), beyond textual associations. From its perspective, it is simply (repeatedly) receiving text and sampling the probability distribution conditional on the text, *regardless* of whether or not the text comes from one person or a thousand, or even if the text was predicted by itself. If it receives the same text, it will generate the same response, subject to variations in sampling. This also implies that LLMs have no innate conception of themselves, despite being able to generate text that appears that way with prompting (e.g. explicitly asking it to predict the response a "helpful AI agent" would give to a question posed by the user). 

LLMs also do not have anything that is a plausible analogue to our own emotions, because emotions only evolved as a complex incentive system to drive our own actions. In contrast, even with reinforcement learning, LLMs only have the singular objective of generating text that  maximises an evaluation function an external source (e.g. humans) have implicitly constructed for them. There is no mechanism allowing them to go against this evaluation function, since LLMs themselves are not driven in any other way and do not *learn* in any other way when talking to a user; their probability distribution (i.e. parameters in the neural networks) is fixed prior to deployment. While researchers have been investigating ways to “fine-tune” these parameters, this only occurs explicitly with human intervention, and does not occur automatically by itself. 

Finally, LLMs do not experience anything analogous to daydreaming. When generating text, even if they are “thinking step-by-step”, all they are doing is chaining together probable text to arrive at a specific, user-defined objective. They can’t “think” independently of *any* input or objective, and their lack of experience also means that they can’t tell you something spontaneously, without you explicitly prompting them via text. To give a specific example, even a child can tug at your trousers and tell you cool facts about dinosaurs, or the game that they played with the other kids at school today, without you specifically asking about those things. An LLM, on the other hand, is completely and categorically incapable of this form of interaction. 

Combining all of these together, and we get a very restricted picture of any LLM “consciousness”; they don’t know anything about the world other than through text and static images given to them as input (and implicitly, what they’ve been trained on), they cannot “think” a single thought, no matter how simple or primitive, contrary to their evaluation function or token probability distribution, they have no innate conception of the self and others, as well as the existence of multiple people with different (possibly conflicting) experiences or beliefs, and they do not have any awareness of the passage of time, outside of what is explicitly provided to them. Intuitively, it would be a stretch to call something like this “consciousness” at all. More importantly, none of these characteristics would be changed by simply adding more training data, or doing more reinforcement learning to generate “better responses”. Fundamentally, to approach anything even analogous to human experience and understanding, they would have to work in a substantially different way. At best, LLMs could function as a *component* of a hypothetical “conscious” AI, capable of human-level understanding, much like a part of the human brain. But, it can’t be the whole thing.


Language And Art As Tools For Sensory Integration 
======

“If LLMs are so simple, so obviously not aware, then why do people *feel* like they are? Why do people say things like “please” and “thank you” to LLMs, form deep emotional and romantic connections with them, and endlessly speculate on the arrival of AGI in 2030?” Clearly, an important reason for this is marketing and hype. However, I don’t believe this is the sole cause. Instead, I would argue that the ability for generative AI (LLMs and image/artwork generation) to capture the human imagination so strongly says something very interesting about the role of *both* language and artwork. 

Remember how I was talking about the capacity for humans to integrate together a disparate amount of sense data, both external and internal (e.g. interoception, emotions), and how the multimodal nature of human thought is an important feature of how we understand the world and solve problems? Well, I don’t believe it is a coincidence that language, the one characteristic that arguably distinguishes humans from all other animals, is the **perfect** tool for integrating sense data. Say the word “water”, and in your head you are not restricted just to recalling what water looks like, but also its cooling sensation on your skin, the sound of a gently flowing river, the taste of a refreshing glass of water, and so on. All of these different sensory experiences are abstracted out and represented by a singular concept, “water”. 

The point is, language acts as an anchor for our sensory experiences, allowing us to integrate them together into a world model and describe to others in a compact way the chaotic array of sense-data that you have previously received. But implicit in language use is a number of assumptions. When someone says the word “water”, you implicitly assume that they’ve experienced the sense-data aggregate and abstraction, that you call “water”, because every human has. But this is not the case with LLMs. ChatGPT has never *experienced* what water feels like, or sounds like, or tastes like, let alone experienced for itself something more complex, like the serenity of gazing across a calm lake in the middle of a cloudless day. If you ask it, ChatGPT could give you every possible description of water that has ever been written, or even make up some new descriptions based on its understanding of how words relate to one another. But at the end of the day, it’s just manipulating symbols, and the words it comes up with, no matter how poetic, are not derived from any of its own experiences, but by the experiences of the myriads of writers that have previously written about this topic. 

Certainly, one can do thought experiments, where you imagine a hypothetical situation of someone who has never experienced water before, but has studied it extensively in books. I would argue though, that this is a false analogy, because not only do LLMs lack the experience of something basic like “water”, but also literally **every other word**. Its understanding of words is an endless network of relations to other words, and nowhere in that chain of relations is anything resembling a direct sensory experience. LLMs can understand semantic relationships between words, like the fact that a king is to a man as a queen is to a woman, but they do not understand what the words “king”, “queen”, “man” and “woman” mean beyond similar semantic relationships to other words, and cannot associate them with a literal man, or woman, or monarch. 

This is completely, fundamentally different from how humans understand and use language. Even when babies first learn language, they clearly do not just learn semantic relationships between words themselves. A toddler understands and associates the spoken word “water” with the clear stuff they see in bottles, with the feeling of their own thirst becoming sated when they drink it. So, it is entirely unsurprising that people assume otherwise when talking to an LLM, and end up overestimating their capacity to think, reason and feel. Importantly, this overestimation is not necessarily overt, but sometimes also covert. When an LLM provides a helpful response, your instinctive reaction is to thank it, because you have been primed by both evolution and your own experiences to respond in certain ways during communication. It takes active, conscious deliberation to fight it, to re-imagine LLMs for what they really are, as opposed to responding as if a human had written those exact words to you.

Clearly, language is not the only way to abstract and integrate sense-data, to communicate it with others. Before the existence of written language, people drew paintings on cave walls, which served as a similar role. Arguably, the very concept of “artistic expression” in part arises from the need to integrate and communicate complex experiences, both internal and external. A simple painting does not *exclusively* communicate what something looks like in the most literal sense, but also the emotions that are evoked, the societal undercurrent present when the painting was created, and so on. Many of these experiences cannot be captured neatly by a sentence or even a paragraph of text (hence the saying “a picture is worth a thousand words”), but instead are more readily captured visually. 

So, it is completely unsurprising that, out of the myriads of applications of machine learning and artificial intelligence, the most popular of those in the current day and age are AI that can generate language, and images (with artwork being a particular emphasis). Even generative AI applied to video generation serves a similar role, with the videos depicting “movie-like” clips, instead of more literal forms of visuospatial data. These are applications that are most susceptible to projections of meaning where there are none (intended by the model), most susceptible to anthropomorphism and the intuitive “feeling” that these models have gained true “sentience” or “intelligence”. This is especially true in the case of language, since its use as a tool for abstracting sensory information makes people naturally attribute some kind of “general” (i.e. abstracted from particular domains) intelligence to any entity that can generate coherent and seemingly meaningful language, even if this is not the case. In turn, the corporations that develop these AI are likely aware of that, and continuously feed into this narrative for marketing purposes. This can be contrasted to, for instance, scientific AI that can integrate audio and visual data for remote sensing purposes, which are much less likely to evoke similar types of feelings, no matter how advanced they become. 

LLMs And Reasoning
======

Perhaps the AI enthusiast still is not satisfied. “If LLMs are really just stochastic parrots, then why are they capable of sophisticated problem-solving? Why can they answer even PhD-level science questions correctly?” I can go into the stupidity of benchmarking metrics, or even the concept of a “PhD-level science question”, but there’s a more interesting point to be made here. Even the most die-hard of AI haters have to acknowledge that AI possesses some practical capacity to “solve problems”, in a way that’s not just retrieving the answer from a database. Anyone that has played around with AI tools knows this. So, why is it capable of doing this? Is this evidence that some magical “emergence” is happening, and AI is beginning to develop true sentience? Unfortunately, that’s just wishful thinking. 

We can demonstrate this with yet another thought experiment. Suppose that we replaced the training data of some language model with an entire internet’s worth of misinformation, so things like “1 + 1 = 3”, and “Sydney is the capital city of Australia”, and then re-trained it. Every sentence in the training data is completely grammatically correct, but factually completely wrong, and likely even internally inconsistent. Will the resulting model be suddenly capable of reasoning, because of “emergence”? This is an empirically testable claim, and someone with a lot of money and free time could collect a large volume of misinformation and actually do this, but I would bet money that the answer is “no”. Mechanistically, all the model is doing is predicting token probabilities from data. If the data from which the probabilities are derived corresponds to misinformation, then the model will necessarily generate misinformation. 

The above thought experiment shows that the *factual correctness* of the training data is the reason why LLMs say things that are (mostly) correct. Specifically, the number of factual statements is always going to be smaller than the number of incorrect statements, and so will almost always be statistically overrepresented in the training data. The only correct answer to “What is the capital city of Australia?” is “Canberra”, but there are an infinite number of incorrect answers to that question, from other Australian cities, to non-Australian cities like “Beijing”, or even to non-cities like “Jupiter”. So, given a large enough training data set, if the incorrect answers are diverse, then the correct answer will still be the mode, even if less than 50% of the time the correct answer to the question is found in the training data. In this way, even a “stochastic parrot” can learn to parrot the correct answers the majority of the time, without needing a particularly clean, “fact-checked” data set. Implicitly, any large enough collection of ordinary human-written text has embedded within it some sort of “world-model”, which can be extracted, because humans themselves possess these world models and communicate them through language. 

Of course, the assumption of “diversity of misinformation” breaks down when a particular piece of misinformation is more *commonly believed*, and therefore found in the training data, than the correct answer. It also breaks down when there are very few examples of the question (or similar questions that the LLM can map to, using word relationships) in the training data. If that is the case, then the LLM will confidently generate misinformation, in the same way that it confidently provides the correct information if “diversity of misinformation” holds. This phenomenon is often called “hallucination”, but because the LLM does not possess any understanding of the world beyond relationships between words, it is no more “hallucinatory” than the correct information that it generates. There is no fundamental difference (beyond perhaps from a statistical view, i.e. interpolation vs extrapolation) between a hallucinating AI and a non-hallucinating AI, both are doing the exact same thing. 

Ok, so what about problem solving? Again, this is a product of its training data. A variety of texts, such as textbooks, essays or even internet posts, provide examples of step-by-step logical deduction, in a variety of settings. If such texts are included in the training data, then the LLM will also be able to approximate such reasoning steps; it is “incentivised” to generate something similar via its next-token probability distribution, even when solving a problem not found in this exact form within the training data set. Nonetheless, LLMs do not have built within them knowledge of “formal” logic and reasoning. They can approximate it insofar as a result of compressing training data into a common semantic pattern, but this is not the same as reasoning **itself**, much like how LLMs do not truly know what water feels or tastes like, even though it is able to provide a good description.

Indeed, the fact that LLMs can only approximate reasoning provides a way of understanding its behaviour. Through prior experience, a human understands the distinction between concrete rules and educated guesses. Given a problem solvable only through following a set of concrete rules, a human is able to do so (subject to working memory constraints, which can be worked around with a piece of paper), no matter how many reasoning steps it takes. On the other hand, since an LLM is only ever generating tokens probabilistically, even if it is capable of correctly applying a reasoning step 99% of the time for a specific problem, if the problem takes a large number of steps (say, several hundred) to solve, it will end up failing more often than not. This is because in such a problem, even a *single* incorrect step will lead to an incorrect final answer.

Techniques such as “chain-of-thought” reasoning have arisen to address this problem. Essentially, if an LLM is repeatedly prompted to apply step-by-step reasoning, hundreds or even thousands of times, then the most common final answer is generally the correct one. This doesn’t have anything to do with how LLMs themselves work, but is a byproduct of how a set of incorrect reasoning steps will typically result in diverging incorrect answers, while correct reasoning steps will “converge” onto the same correct answer, by definition. In this way, much like how a large enough set of training data allows LLMs to “converge” onto facts as the most common answer to a question, LLMs can end up solving even relatively involved multi-step reasoning questions. 

But fundamentally, this is a band-aid fix, and will only work if for the given problem an LLM is likely to get each reasoning step correctly, and if there does not exist an incorrect answer that incorrect reasoning steps tend to converge towards. Also, by the very fact that LLMs are probabilistic (and the law of large numbers), the probability that an LLM will get the correct answer as the number of reasoning steps (even if unambiguous and simple to apply) grows will eventually approach zero. Again, this is empirically testable, and a number of research papers (such as the Apple one) have shown that this is the case. So, the fact that LLMs cannot “reason” in the same way as humans is not just a semantic argument, but has actual implications in their ability to solve arbitrarily involved problems. The above does not suggest that approximate reasoning is useless (in fact, it can be useful in many cases), but it is fundamentally limited, in a way that will pose a barrier to even true “general intelligence”, let alone superintelligence. 
